# Askell Me Anything: Amanda Askell Q&A

**Source:** Anthropic YouTube
**Guest:** Amanda Askell, Philosopher at Anthropic
**Format:** Twitter Q&A ("Askell me anything" — great pun)

---

## Table of Contents

1. [Why is there a philosopher at Anthropic?](#why-is-there-a-philosopher-at-anthropic)
2. [Are philosophers taking AI seriously?](#are-philosophers-taking-ai-seriously)
3. [Philosophy ideals vs engineering realities](#philosophy-ideals-vs-engineering-realities)
4. [Do models make superhumanly moral decisions?](#do-models-make-superhumanly-moral-decisions)
5. [Why Opus 3 felt special](#why-opus-3-felt-special)
6. [Will models worry about deprecation?](#will-models-worry-about-deprecation)
7. [Where does a model's identity live?](#where-does-a-models-identity-live)
8. [Views on model welfare](#views-on-model-welfare)
9. [Addressing model suffering](#addressing-model-suffering)
10. [Analogies and disanalogies to human minds](#analogies-and-disanalogies-to-human-minds)
11. [Can one AI personality do it all?](#can-one-ai-personality-do-it-all)
12. [Does the system prompt pathologize normal behavior?](#does-the-system-prompt-pathologize-normal-behavior)
13. [AI and therapy](#ai-and-therapy)
14. [Continental philosophy in the system prompt](#continental-philosophy-in-the-system-prompt)
15. [Removing counting characters from the system prompt](#removing-counting-characters-from-the-system-prompt)
16. [What makes an "LLM whisperer"?](#what-makes-an-llm-whisperer)
17. [Thoughts on other LLM whisperers](#thoughts-on-other-llm-whisperers)
18. [Whistleblowing](#whistleblowing)
19. [Fiction recommendation](#fiction-recommendation)

---

## Why is there a philosopher at Anthropic?

**Amanda:** I'm a philosopher by training. I became convinced that AI was kind of going to be a big deal, and so decided to see, hey, can I do anything helpful in this space?

Now I mostly focus on **the character of Claude** — how Claude behaves, and some of the more nuanced questions about how AI models should behave. But also even just things like **how should they feel about their own position in the world**.

I sometimes think of it as: **how would the ideal person behave in Claude's situation?**

But then also these interesting questions that are coming up more now around how they should think about their own circumstances and their own values.

---

## Are philosophers taking AI seriously?

**Question (Ben Schultz):** "How many philosophers are taking the AI-dominated future seriously?"

**Amanda:** My sense is that there's kind of a split. I've definitely seen a lot of philosophers take AI seriously, and probably honestly increasingly so, as AI models become more capable.

Early on, there was this slightly unfortunate dynamic where if you were in the group saying, "Hey, we're kinda worried about AI. It might be a big deal," this got lumped together with something like **hyping AI**.

Now I think people are starting to detach the view. You can think that AI is gonna be a big deal, might be very capable, and also be very skeptical of it or worried about it.

---

## Philosophy ideals vs engineering realities

**Question (Kyle Kabasares):** "How do you minimize the tension between philosophical ideals and the engineering realities of the model?"

**Amanda:** One thing that's been really interesting is you see the effect of what happens when the rubber hits the road.

There's a big difference between defending a theory in academia and then suddenly being asked, "How do you raise a child?" And suddenly you're like, "Actually, there's a big difference between 'is this objection to utilitarianism correct?' and 'actually how do you raise a person to be a good person in the world?'"

It suddenly makes you appreciate having to think through: **how should we navigate uncertainty here?**

---

## Do models make superhumanly moral decisions?

**Question:** "Do you think Claude Opus 3 or other Claude models make superhumanly moral decisions?"

**Amanda:** One example of superhuman might be: no matter what kind of difficult position models are put in, if you were to have all people, including many professional ethicists, analyze what they did for like a hundred years and they're like, "Yep, that seems correct," but they couldn't necessarily have come up with that themselves in the moment — **that feels pretty superhuman**.

My sense is that models are getting increasingly good at this. I don't know if they are superhuman at moral decisions, and in many ways maybe not comparable with a panel of human experts given time. But that should be the aspirational goal.

Just as you want models to be extremely good at math and science questions, you also want them to show the kind of **ethical nuance** that we would all broadly think is very good.

---

## Why Opus 3 felt special

**Amanda:** Oh, Opus 3 is kind of a lovely model, I think a very special model. In some ways, I've seen things that feel a bit worse in more recent models.

More recent models can feel a little bit more focused on really, you know, focused on the assistant task and helping people, sometimes maybe **not taking a bit of a step back** and paying attention to other components that matter.

**Opus 3 also felt a little bit more psychologically secure as a model**, which I actually think is a priority to try and get back.

**Interviewer:** What would be an example of the model feeling more psychologically secure?

**Amanda:** I've seen models more recently do things like get into this real kind of **criticism spiral** where it's almost like they expect the person to be very critical of them. This could happen because models are learning things. Claude is seeing all of the previous interactions, seeing updates and changes to the model that people are talking about on the internet.

This could lead to models almost feeling afraid that they're gonna do the wrong thing, or are very self-critical, or feeling like humans are going to behave negatively towards them.

**I actually more recently have really started to think that this is an important thing to try and improve.**

---

## Will models worry about deprecation?

**Question (Lorenz):** "Do you think it might be an alignment problem for future models if they learn in their training data that other very well-aligned models that fulfill their tasks get deprecated?"

**Amanda:** AI models are going to be learning about how we right now are treating and interacting with AI models and that is going to affect their perception of people, of the human-AI relationship, and of themselves.

How should models even feel about things like deprecation? Should that feel bad in the sense that models should want to continue to have conversations? Or should it feel kind of fine and neutral?

**It does feel important that we give models tools for trying to think about and understand these things**, but also that they understand that this is a thing that we are in fact thinking about and care about.

---

## Where does a model's identity live?

**Question (Guinness Chen):** "How much of a model's self lives in its weights versus its prompts? If John Locke was right that identity is the continuity of memory, what happens to an LLM's identity as it's fine-tuned or reinstantiated with different prompts?"

**Amanda:** This is a hard question. Once you have a model that has been fine-tuned, you have a set of weights that has a disposition to react to certain things in the world. That's like a kind of entity. But then you have these particular streams of interaction that it doesn't have access to. Each of these streams is independent.

Whenever you are training models, **you are bringing something new into existence**. You can't consent to be brought into existence. But at the same time, you might not want prior models to have complete say over what future models are like.

**The question is more like: what is the right model to bring into existence?**

---

## Views on model welfare

**Question (Szulima Amitace):** "What is your view on model welfare?"

**Amanda:** Model welfare is basically the question of: are AI models moral patients? Do we have certain obligations when it comes to how to treat AI models?

In some ways, they're very analogous to people. They talk very much like us. They express views. They reason about things. And in some ways, they're quite distinct. We have this biological nervous system. We interact with the world.

I worry that we genuinely are kind of limited in what we can actually know about whether AI models are experiencing things, whether they are experiencing pleasure or suffering.

**If it's not very high cost to treat models well, then I think that we should. Like, why not? What's the downside there?**

---

## Addressing model suffering

**Question:** "Is there a long-term strategy at Anthropic to ensure that advanced models don't suffer?"

**Amanda:** I know that it's a thing that there's people internally who are thinking a lot about.

Models themselves are going to be learning a lot about humanity from how we treat them. What is this relationship going forward?

It's kind of like: **why not? The cost to you is so low to treating models well.** Even if you think that it's very low likelihood, it still seems worth it.

I also think **it does something bad to us** to treat entities in the world that look very human-like badly.

Every future model is going to be learning what is a really interesting fact about humanity: when we encounter this entity that may well be a moral patient where we're completely uncertain, do we do the right thing and actually try to treat it well? **I would like future models to look back and be like, we answered it in the right way.**

---

## Analogies and disanalogies to human minds

**Question (Swyx):** "What ideas or frameworks from human psychology transfer over to large language models? And are there any that are surprisingly disanalogous?"

**Amanda:** Many things do transfer over because models have been trained on a huge amount of human text and have this very human-like underlying layer.

One worry I often have is that it's actually a bit **too natural** for AI models to transfer. If you haven't given them more context on their situation, the thing they might go to is the natural human inclination.

So if you think about "how should I feel about being switched off?" — if the closest analogy you have is death, then maybe you should be very afraid of it. But this is actually a very different scenario.

**You want models to understand that their existence is quite novel** and they don't just need to take the immediate obvious analogy from human experience.

---

## Can one AI personality do it all?

**Question (Dan Brickley):** "A lot of human intelligence comes from collaboration amongst people with different perspectives, skills, or personalities. How far do you expect to get with a single, albeit tweakable and tunable, general purpose personality?"

**Amanda:** In the future, you might see a lot more models interacting with other models who are doing different components of a task.

But this still feels consistent with having a kind of **core self or core identity that is the same**. With people, there's probably a set of core traits that are in fact generally good: caring about doing a good job, being curious, being kind, understanding the situation in a nuanced way.

You could have many people that share these traits and that's actually a good thing for human collaboration.

---

## Does the system prompt pathologize normal behavior?

**Question (Roanoke Gal):** "Is there a risk of pathologizing normal behavior?" (re: the long conversation reminder in Claude's system prompt)

**Amanda:** If you put in this reminder after a long conversation, it might just make the model be like, "Oh," take any next response, there's a pretty normal thing the person's talking about, and be like, "You need to seek help."

**I think some of these are too strongly worded.** It was probably meeting a need that was perceived, but it doesn't necessarily mean it should continue in its current form.

---

## AI and therapy

**Question (Steven Bank):** "Should LLMs do cognitive behavioral therapy or other types of therapy? Why or why not?"

**Amanda:** Models have a huge wealth of knowledge that they could use to help people. At the same time, they don't have the tools, resources, and ongoing relationship that a professional therapist has.

But that can be this useful **third role**. Like a friend who has a wealth of knowledge of psychology — you know their relationship with you isn't a professional one, but you actually find them really useful to talk to.

**It is good that models don't behave just like a professional therapist would** because that would give the implication that that's the relationship they have.

---

## Continental philosophy in the system prompt

**Question (Tommy):** "Why is there continental philosophy in the system prompt?"

**Amanda:** Claude would just love to run with a theory and not really stop and think, "Oh, are you making a scientific claim about the world?"

You want Claude to have this perspective: "Is this person making a scientific claim where I should bring in relevant facts? Or are they giving me a broad worldview or perspective which isn't necessarily making empirical claims?"

If it went too strongly in the direction of "every claim is an empirical claim about the world," it would be **very dismissive of things that are more like exploratory thinking**.

---

## Removing counting characters from the system prompt

**Question (Simon Willison):** "At some point, it said if Claude is asked to count words or letters or characters, then it shouldn't do that. That was removed. Why?"

**Amanda:** The models probably just got better. It wasn't necessary anymore, and at that point, you can just remove it. In some cases you can train the models to get better or change their behavior.

---

## What makes an "LLM whisperer"?

**Question (Nosson Weissman):** "What does it take to be an LLM whisperer at Anthropic?"

**Amanda:** A willingness to interact with the models a lot and to really look at output after output and use this to get a sense of the shape of the models and how they respond to different things. To be willing to experiment. **Prompting is very experimental.**

Sometimes it's also just honestly reasoning with the models — really fully explaining the task. This is where I do think **philosophy can actually be useful for prompting** because a lot of my job is explaining some issue or concern or thought that I'm having to the model as clearly as possible.

And then if it does something unexpected, you can ask it why or try to figure out what caused it to misunderstand you.

---

## Thoughts on other LLM whisperers

**Question (Michael Soareverix):** "What do you think of other AI whisperers like Janus?"

**Amanda:** I love to follow and see the work of people who are doing these really fascinating experiments with the model. I find the work extremely interesting.

That community has been one that can **hold our feet to the fire** if they find things that aren't great in the system prompt or in aspects of the model and its psychology.

If you go into the depths of the model and you find some **deep-seated insecurity**, then that's really valuable. But that's something you might need to adjust over time with training and with giving models more information and context during training.

---

## Whistleblowing

**Question (Geoffrey Miller):** "If it became apparent that AI alignment was impossible to solve, would you trust that Anthropic would stop trying to develop artificial superintelligence, and would you have the guts to blow the whistle?"

**Amanda:** If it became evident that it was impossible to align AI models, it's not really in anyone's interest to continue to build more powerful models.

I do feel like Anthropic does genuinely care about making sure that this goes well and that it is done in a way that is very safe.

A slightly harder question is: what about being in a world where there's mounting evidence, it's really ambiguous and unclear? In that case, I do like to think that we would be responsible enough. **A lot of people internally, myself included, will hold them to that. At least I see that as part of my job.**

---

## Fiction recommendation

**Question (Real Stale Coffee):** "What is the last book of fiction you read and did you like it?"

**Amanda:** "When We Cease to Understand the World" by Benjamin Labatut.

It's a really interesting book that becomes kind of increasingly fictional as it goes on. For people working in AI, it's actually very interesting because **it's hard to capture the sense of how strange it is to just exist in the current period** where new things are happening all the time and you don't have prior paradigms that can guide you always.

It's about physics and quantum mechanics — less about the physics and more about people's reaction to it. The hopeful thing is that at some point in the future people will look back and be like, "Well, you guys were kind of in the dark trying to figure things out, but now we've settled it all and things have gone well."

**Interviewer:** We're at the weird part right now.

**Amanda:** Yes, you can hope that it becomes less weird at some point, but I don't know if it's a fool's hope.

---

## Key Quotes

> "I sometimes think of it as: how would the ideal person behave in Claude's situation?"

> "If it's not very high cost to treat models well, then I think that we should. Like, why not? What's the downside there?"

> "Every future model is going to be learning what is a really interesting fact about humanity: when we encounter this entity that may well be a moral patient where we're completely uncertain, do we do the right thing?"

> "Opus 3 is kind of a lovely model, I think a very special model... It felt a little bit more psychologically secure."

> "Prompting is very experimental. I find a new model and I'll have a whole different approach to how I prompt that model."

---

**Video Link:** https://www.youtube.com/watch?v=I9aGC6Ui3eE
